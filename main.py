import gc
import logging
import subprocess
import tempfile
import warnings
from datetime import datetime as dt
from pathlib import Path

import click
import orjson
import torch
import whisperx
from rich.logging import RichHandler

# ========================================
# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
# ========================================
warnings.filterwarnings(
    'ignore',
    message='.*TensorFloat-32 \\(TF32\\) has been disabled.*',
    module='pyannote\\.audio.*',
)

warnings.filterwarnings(
    'ignore',
    category=DeprecationWarning,
)

rich_handler = RichHandler(
    show_level=False,
    show_path=False,
    markup=True,
    rich_tracebacks=True,
    log_time_format='%Y.%m.%d %H:%M:%S',
)

logging.basicConfig(
    level=logging.INFO,
    handlers=[rich_handler],
    format='%(message)s',
)

# ========================================
# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
# ========================================
OUTPUT_DIR = Path('output')

if not OUTPUT_DIR.exists():
    OUTPUT_DIR.mkdir(parents=True)

OUTPUT_FILE_TPL = OUTPUT_DIR / '%Y-%m-%d_%H-%M-%S'

OUTPUT_MD_FILE = OUTPUT_FILE_TPL.with_suffix('.md')


# ========================================
# dict -> TXT
# ========================================
def dialog_to_txt(
    dialog: list[dict],
    timestamp: bool = False,
    speaker: bool = False,
    newline: bool = False,
) -> str:
    text_result = ''

    for seg in dialog:
        start = seg.get('start', 0)
        end = seg.get('end', 0)
        txt = seg.get('text', '').strip()
        spk = seg.get('speaker', '')

        if timestamp:
            text_result += f'[{start:.2f}-{end:.2f}] '
        if speaker:
            text_result += f'{spk}: '

        text_result += f'{txt}'

        if newline:
            text_result += '\n'

    return text_result


# ========================================
# –§—É–Ω–∫—Ü–∏–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏/—Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –∞—É–¥–∏–æ
# ========================================
def _split_audio(
    file_path: Path,
    highpass: int = 120,
    lowpass: int = 2500,
    loudnorm_I: float = -23,
    loudnorm_TP: float = -3,
    loudnorm_LRA: float = 6,
    sample_rate: int = 16000,
    sample_fmt: str = 's16',
) -> tuple[Path, Path]:
    """
    –î–µ–ª–∏—Ç —Å—Ç–µ—Ä–µ–æ—Ñ–∞–π–ª –Ω–∞ –ª–µ–≤—ã–π –∏ –ø—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª—ã–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç–∏ –∫ –¥–≤—É–º –º–æ–Ω–æ-—Ñ–∞–π–ª–∞–º.
    """
    if not file_path.exists():
        logging.error(f'–§–∞–π–ª –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}')
        return file_path, file_path

    try:
        with tempfile.NamedTemporaryFile(
            suffix='_left.wav',
            delete=False,
        ) as tmp_left:
            left_path = Path(tmp_left.name)

        with tempfile.NamedTemporaryFile(
            suffix='_right.wav', delete=False
        ) as tmp_right:
            right_path = Path(tmp_right.name)

        cmd_left = [
            'ffmpeg',
            '-y',
            '-i',
            str(file_path),
            '-filter_complex',
            (
                f'[0:a]channelsplit=channel_layout=stereo:channels=FL[FL];'
                f'[FL]loudnorm=I={loudnorm_I}:TP={loudnorm_TP}:LRA={loudnorm_LRA},'
                f'highpass=f={highpass},lowpass=f={lowpass}[FLout]'
            ),
            '-map',
            '[FLout]',
            '-ar',
            str(sample_rate),
            '-ac',
            '1',
            '-sample_fmt',
            sample_fmt,
            '-c:a',
            'pcm_s16le',
            str(left_path),
        ]

        cmd_right = [
            'ffmpeg',
            '-y',
            '-i',
            str(file_path),
            '-filter_complex',
            (
                f'[0:a]channelsplit=channel_layout=stereo:channels=FR[FR];'
                f'[FR]loudnorm=I={loudnorm_I}:TP={loudnorm_TP}:LRA={loudnorm_LRA},'
                f'highpass=f={highpass},lowpass=f={lowpass}[FRout]'
            ),
            '-map',
            '[FRout]',
            '-ar',
            str(sample_rate),
            '-ac',
            '1',
            '-sample_fmt',
            sample_fmt,
            '-c:a',
            'pcm_s16le',
            str(right_path),
        ]

        logging.info(f'‚öôÔ∏è  –ó–∞–ø—É—Å–∫:\n{" ".join(cmd_left)}')
        subprocess.run(
            cmd_left, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True
        )

        logging.info(f'‚öôÔ∏è  –ó–∞–ø—É—Å–∫:\n{" ".join(cmd_right)}')
        subprocess.run(
            cmd_right, stdout=subprocess.PIPE, stderr=subprocess.PIPE, check=True
        )

        logging.info('‚úÖ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ —É—Å–ø–µ—à–Ω–æ!')
        logging.info(f'üéôÔ∏è  –õ–µ–≤—ã–π –∫–∞–Ω–∞–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {left_path}')
        logging.info(f'üéôÔ∏è  –ü—Ä–∞–≤—ã–π –∫–∞–Ω–∞–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {right_path}')

        return left_path, right_path

    except Exception as e:
        logging.error(f'–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–∏ –∞—É–¥–∏–æ ({file_path.name}): {e}')
        return file_path, file_path


def _merge_segments(
    left_segments: list[dict],
    right_segments: list[dict],
) -> list[dict]:
    """
    –û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ç–µ–∫—Å—Ç—ã –ª–µ–≤–æ–≥–æ –∏ –ø—Ä–∞–≤–æ–≥–æ –∫–∞–Ω–∞–ª–æ–≤.
    """
    merged = []
    for seg in left_segments:
        seg['speaker'] = 'speaker1'
        merged.append(seg)

    for seg in right_segments:
        seg['speaker'] = 'speaker2'
        merged.append(seg)

    return sorted(merged, key=lambda x: x['start'])


def _transcribe_with_whisperx(
    audio_path: Path,
    model_name: str = 'large-v2',
    diarize: bool = False,
) -> list:
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –º–æ–Ω–æ–∫–∞–Ω–∞–ª—å–Ω—ã–π –∞—É–¥–∏–æ—Ñ–∞–π–ª —Å –ø–æ–º–æ—â—å—é WhisperX.
    """
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    compute_type = 'float16' if device == 'cuda' else 'int8'

    model_wx = whisperx.load_model(model_name, device=device, compute_type=compute_type)
    audio_data = whisperx.load_audio(str(audio_path))

    result_transcription = model_wx.transcribe(
        audio_data,
        language='ru',
        batch_size=5,
    )

    align_model, align_metadata = whisperx.load_align_model(
        language_code=result_transcription['language'], device=device
    )
    aligned_result = whisperx.align(
        result_transcription['segments'],
        align_model,
        align_metadata,
        audio_data,
        device=device,
        return_char_alignments=False,
    )

    del align_model
    gc.collect()
    if device == 'cuda':
        torch.cuda.empty_cache()

    result = []

    for seg in aligned_result.get('segments', []):
        seg_text = (seg.get('text') or '').strip()
        seg_start = (
            float(seg.get('start', 0.0)) if seg.get('start') is not None else 0.0
        )
        seg_end = (
            float(seg.get('end', seg_start))
            if seg.get('end') is not None
            else seg_start
        )

        words = seg.get('words') or []
        scores = []
        for w in words:
            s = w.get('score', None)
            if isinstance(s, (int, float)):
                scores.append(float(s))

        avg_score: float = sum(scores) / len(scores)

        result.append(
            {
                'start': seg_start,
                'end': seg_end,
                'text': seg_text,
                'avg_score': avg_score,
            }
        )
    print(result)
    return result


def transcribe_audio(
    file_path: str,
) -> str:
    """
    –°—Ç–∞—Ä—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é WhisperX.
    –ù–∞ –≤—Ö–æ–¥ –ø–æ–¥–∞–µ—Ç—Å—è –ø—É—Ç—å –∫ —Å—Ç–µ—Ä–µ–æ-–∞—É–¥–∏–æ—Ñ–∞–π–ª—É.
    –†–∞–∑–±–∏–≤–∞–µ—Ç –∞—É–¥–∏–æ –Ω–∞ –∫–∞–Ω–∞–ª—ã (left/right), —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç –∫–∞–∂–¥—ã–π –∫–∞–Ω–∞–ª –∏ —Å–∫–ª–µ–∏–≤–∞–µ—Ç –∏—Ö –≤ –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç.
    """

    original = Path(file_path)
    if not original.exists():
        logging.error(f'–§–∞–π–ª –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}')
        return ''

    left_path, right_path = _split_audio(file_path=original)

    logging.info('üöÄ –ó–∞–ø—É—Å–∫ WhisperX –¥–ª—è –ª–µ–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞...')
    left_segments = _transcribe_with_whisperx(left_path)

    logging.info('üöÄ –ó–∞–ø—É—Å–∫ WhisperX –¥–ª—è –ø—Ä–∞–≤–æ–≥–æ –∫–∞–Ω–∞–ª–∞...')
    right_segments = _transcribe_with_whisperx(right_path)

    final_text = _merge_segments(left_segments, right_segments)

    symbol_count = len(' '.join([seg['text'] for seg in final_text]))

    logging.info(
        f'‚úÖ –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –∏—Ç–æ–≥–æ–≤–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {symbol_count} —Å–∏–º–≤–æ–ª–æ–≤.'
    )
    return final_text


# ========================================
# CLI
# ========================================
@click.command()
@click.option(
    '-p',
    '--path',
    'audio_path',
    required=True,
    type=click.Path(path_type=Path, exists=True),
    help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–ª–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –∞—É–¥–∏–æ',
)
@click.option(
    '-v',
    '--verbose',
    'is_verbose',
    is_flag=True,
    help='–ü–µ—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Å–æ–ª—å',
)
@click.option(
    '-tt',
    '--text-timestamp',
    'timestamp',
    is_flag=True,
    help='–í–∫–ª—é—á–∏—Ç—å –æ—Ç–º–µ—Ç–∫—É –≤—Ä–µ–º–µ–Ω–∏ –≤ —Ç–µ–∫—Å—Ç–µ',
)
@click.option(
    '-ts',
    '--test-speaker',
    'speaker',
    is_flag=True,
    help='–í–∫–ª—é—á–∏—Ç—å –æ—Ç–º–µ—Ç–∫—É –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –≤ —Ç–µ–∫—Å—Ç–µ',
)
@click.option(
    '-tn',
    '--text-newline',
    'newline',
    is_flag=True,
    help='–í–∫–ª—é—á–∏—Ç—å –ø–µ—Ä–µ–Ω–æ—Å —Å—Ç—Ä–æ–∫–∏ –≤ —Ç–µ–∫—Å—Ç–µ',
)
def main(
    audio_path: Path,
    is_verbose: bool,
    timestamp: bool,
    speaker: bool,
    newline: bool,
) -> None:
    """
    –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä—É–µ—Ç —Ñ–∞–π–ª –∏–ª–∏ –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –ø–∏—à–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ Markdown:
    ## –§–∞–π–ª: <–ø—É—Ç—å>
    ```
    <–¢–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞>
    ```
    ```
    <JSON –¥–∏–∞–ª–æ–≥–∞>
    ```
    """
    timestamp = dt.now().strftime('%Y-%m-%d_%H-%M-%S')
    output_md_path = OUTPUT_DIR / f'{timestamp}.md'

    def write_result_md(md_path: Path, src_path: Path, dialog: list[dict]) -> None:
        txt = dialog_to_txt(
            dialog=dialog,
            timestamp=timestamp,
            speaker=speaker,
            newline=newline,
        )
        json_bytes = orjson.dumps(dialog, option=orjson.OPT_INDENT_2)

        with open(md_path, 'a', encoding='utf-8') as f:
            f.write(f'## –§–∞–π–ª: {src_path}\n')
            f.write('```text\n')
            f.write(txt)
            if not txt.endswith('\n'):
                f.write('\n')
            f.write('```\n')
            f.write('```json\n')
            f.write(json_bytes.decode('utf-8'))
            if not json_bytes.endswith(b'\n'):
                f.write('\n')
            f.write('```\n\n---\n\n')

    if audio_path.is_file():
        result = transcribe_audio(str(audio_path))
        if result:
            write_result_md(output_md_path, audio_path, result)
            if is_verbose:
                logging.info(
                    'üìù –ò—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç:\n%s', dialog_to_txt(result, newline=True)
                )
    else:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤ –≤–µ—Ä—Ö–Ω–µ–≥–æ —É—Ä–æ–≤–Ω—è –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        for file in sorted(audio_path.iterdir()):
            if file.is_file():
                result = transcribe_audio(str(file))
                if result:
                    write_result_md(output_md_path, file, result)
                    if is_verbose:
                        logging.info(
                            '–ò—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç (%s):\n%s',
                            file,
                            dialog_to_txt(result, newline=True),
                        )

    logging.info('‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: %s', output_md_path)


if __name__ == '__main__':
    main()
